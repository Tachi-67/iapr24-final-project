{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Author:**\n",
    "\n",
    "Haolong Li (haolong.li@epfl.ch) (352680)\n",
    "\n",
    "Zhibo Zhao (zhibo.zhao@epfl.ch) (350593)\n",
    "\n",
    "**NOTE:** It is NOT expected to run this notebook, because the training data is too big to upload. We upload this notebook just in case of integrity.\n",
    "\n",
    "**Refer to `classify.ipynb` for the kaggle-solution reproduce pipeline.**\n",
    "\n",
    "The result of the training (model weights) is stored in `model/best_model.pth`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import json\n",
    "import numpy as np\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_xml(xml_file):\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "    annotations = []\n",
    "    for member in root.findall('object'):\n",
    "        value = member.find('name').text\n",
    "        bndbox = member.find('bndbox')\n",
    "        xmin = int(bndbox.find('xmin').text)\n",
    "        ymin = int(bndbox.find('ymin').text)\n",
    "        xmax = int(bndbox.find('xmax').text)\n",
    "        ymax = int(bndbox.find('ymax').text)\n",
    "        annotations.append((value, (xmin, ymin, xmax, ymax)))\n",
    "    return annotations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('5CHF', (1588, 1605, 2511, 2516)),\n",
       " ('2EUR', (3011, 1311, 3774, 2079)),\n",
       " ('0.2EUR', (2574, 2342, 3158, 2932)),\n",
       " ('0.5EUR', (3132, 2884, 3716, 3474)),\n",
       " ('0.2EUR', (1868, 2963, 2505, 3532))]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Looking at one parsed XML file\n",
    "xml_file = os.path.join('data', 'train',  'L1010277.xml')\n",
    "res = parse_xml(xml_file)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0.05EUR': 0, '1EUR': 1, '5CHF': 2, '0.02EUR': 3, '1CHF': 4, '0.01EUR': 5, '2CHF': 6, '0.2CHF': 7, '0.05CHF': 8, '0.5CHF': 9, '0.5EUR': 10, '2EUR': 11, '0.1EUR': 12, '0.1CHF': 13, 'OOD': 14, '0.2EUR': 15}\n"
     ]
    }
   ],
   "source": [
    "# label string to integer mapping\n",
    "def collect_labels(xml_dir):\n",
    "    labels = set()\n",
    "    for xml_file in os.listdir(xml_dir):\n",
    "        if xml_file.endswith('.xml'):\n",
    "            tree = ET.parse(os.path.join(xml_dir, xml_file))\n",
    "            root = tree.getroot()\n",
    "            for member in root.findall('object'):\n",
    "                labels.add(member.find('name').text)\n",
    "    return labels\n",
    "\n",
    "label_set = collect_labels(os.path.join('data', 'train'))\n",
    "label_to_index = {label: idx for idx, label in enumerate(label_set)}\n",
    "\n",
    "print(label_to_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the dataset class\n",
    "class CoinDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        # temp root = data/temp_train\n",
    "        # temp anno = data/temp_train_annotations\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.img_files = [os.path.join(root_dir, file) for file in os.listdir(root_dir) if file.endswith('.JPG')] # list of all image files\n",
    "        self.annotations = [parse_xml(file.replace('.JPG', '.xml')) for file in self.img_files] # list of lists, each list contains tuples of value and bounding box coordinates\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_files[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        annotations = self.annotations[idx]\n",
    "        \n",
    "        coins = []\n",
    "        for value, (xmin, ymin, xmax, ymax) in annotations:\n",
    "            coin = image.crop((xmin, ymin, xmax, ymax))\n",
    "            if self.transform:\n",
    "                coin = self.transform(coin)\n",
    "            coins.append((coin, value))\n",
    "        \n",
    "        return coins\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "dataset = CoinDataset(root_dir=os.path.join('data', 'train'), transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    images = []\n",
    "    labels = []\n",
    "    # 遍历batch中的每一个元素，每个元素都是一个图片中的多个硬币\n",
    "    for item in batch:\n",
    "        for coin, label in item:\n",
    "            images.append(coin)           # 添加每个硬币的图像\n",
    "            labels.append(label_to_index[label])  # 使用映射转换标签\n",
    "\n",
    "    # 将images列表转换为一个tensor，labels列表转换为一个tensor\n",
    "    images = torch.stack(images)\n",
    "    labels = torch.tensor(labels, dtype=torch.int64)\n",
    "\n",
    "    return images, labels\n",
    "\n",
    "\n",
    "\n",
    "def train_val_split(dataset, val_split=0.2):\n",
    "    indices = np.arange(len(dataset))\n",
    "    np.random.shuffle(indices)\n",
    "    split = int(np.floor(val_split * len(indices)))\n",
    "    train_indices, val_indices = indices[split:], indices[:split]\n",
    "    \n",
    "    train_dataset = Subset(dataset, train_indices)\n",
    "    val_dataset = Subset(dataset, val_indices)\n",
    "    \n",
    "    return train_dataset, val_dataset\n",
    "\n",
    "\n",
    "train_dataset, val_dataset = train_val_split(dataset, val_split=0.1)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 加载预训练的 ResNet50 模型\n",
    "model = models.resnet50(weights='IMAGENET1K_V2')\n",
    "\n",
    "# 修改最后一个全连接层以匹配硬币分类的类别数\n",
    "num_classes = len(label_to_index)  # 假设你已经有了标签索引\n",
    "model.fc = torch.nn.Linear(model.fc.in_features, num_classes)\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model, data_loader, criterion):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "    return val_loss / len(data_loader)\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=5):\n",
    "    best_val_loss = float('inf')\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        # 计算验证损失\n",
    "        val_loss = validate_model(model, val_loader, criterion)\n",
    "        print(f'Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}, Validation Loss: {val_loss}')\n",
    "        \n",
    "        # 检查是否有最佳模型，并保存\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "            print(f\"Saved better model with validation loss: {val_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 2.740930271148682, Validation Loss: 2.7348225116729736\n",
      "Saved better model with validation loss: 2.7348225116729736\n",
      "Epoch 2, Loss: 2.39723653793335, Validation Loss: 2.633789539337158\n",
      "Saved better model with validation loss: 2.633789539337158\n",
      "Epoch 3, Loss: 2.0391609191894533, Validation Loss: 2.408545732498169\n",
      "Saved better model with validation loss: 2.408545732498169\n",
      "Epoch 4, Loss: 1.6688083171844483, Validation Loss: 2.1467816829681396\n",
      "Saved better model with validation loss: 2.1467816829681396\n",
      "Epoch 5, Loss: 1.3558610200881958, Validation Loss: 1.8719234466552734\n",
      "Saved better model with validation loss: 1.8719234466552734\n",
      "Epoch 6, Loss: 1.049644160270691, Validation Loss: 1.673026204109192\n",
      "Saved better model with validation loss: 1.673026204109192\n",
      "Epoch 7, Loss: 0.7817895650863648, Validation Loss: 1.4931954145431519\n",
      "Saved better model with validation loss: 1.4931954145431519\n",
      "Epoch 8, Loss: 0.5447116613388061, Validation Loss: 1.3262101411819458\n",
      "Saved better model with validation loss: 1.3262101411819458\n",
      "Epoch 9, Loss: 0.37195721864700315, Validation Loss: 1.2384984493255615\n",
      "Saved better model with validation loss: 1.2384984493255615\n",
      "Epoch 10, Loss: 0.25723162591457366, Validation Loss: 1.1847167015075684\n",
      "Saved better model with validation loss: 1.1847167015075684\n",
      "Epoch 11, Loss: 0.169619420170784, Validation Loss: 1.122215747833252\n",
      "Saved better model with validation loss: 1.122215747833252\n",
      "Epoch 12, Loss: 0.10901663675904275, Validation Loss: 1.07761812210083\n",
      "Saved better model with validation loss: 1.07761812210083\n",
      "Epoch 13, Loss: 0.07248848006129265, Validation Loss: 1.0564203262329102\n",
      "Saved better model with validation loss: 1.0564203262329102\n",
      "Epoch 14, Loss: 0.04284987524151802, Validation Loss: 1.0314831733703613\n",
      "Saved better model with validation loss: 1.0314831733703613\n",
      "Epoch 15, Loss: 0.034639110788702965, Validation Loss: 1.015237808227539\n",
      "Saved better model with validation loss: 1.015237808227539\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# 现在开始训练\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.02251990996301174, Validation Loss: 0.9925457835197449\n",
      "Saved better model with validation loss: 0.9925457835197449\n",
      "Epoch 2, Loss: 0.0176901800557971, Validation Loss: 0.9894407987594604\n",
      "Saved better model with validation loss: 0.9894407987594604\n",
      "Epoch 3, Loss: 0.016770420409739018, Validation Loss: 1.0101858377456665\n",
      "Epoch 4, Loss: 0.018623148463666438, Validation Loss: 1.0089820623397827\n",
      "Epoch 5, Loss: 0.008884755708277226, Validation Loss: 1.024297833442688\n",
      "Epoch 6, Loss: 0.009139344561845064, Validation Loss: 1.0353553295135498\n",
      "Epoch 7, Loss: 0.007786692492663861, Validation Loss: 1.0406599044799805\n",
      "Epoch 8, Loss: 0.0076099943369627, Validation Loss: 1.0470120906829834\n",
      "Epoch 9, Loss: 0.006128749437630176, Validation Loss: 1.0534926652908325\n",
      "Epoch 10, Loss: 0.006275934912264347, Validation Loss: 1.0614908933639526\n"
     ]
    }
   ],
   "source": [
    "# do more trainings, as we haven't seen overfitting yet\n",
    "\n",
    "# load model\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "model = model.to(device)\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the label to index mapping\n",
    "\n",
    "with open('label_to_index.json', 'w') as f:\n",
    "    json.dump(label_to_index, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: 5CHF, Probability: 0.9047756791114807\n"
     ]
    }
   ],
   "source": [
    "model.eval()  # 设置模型为评估模式\n",
    "\n",
    "def predict_image(image_path, model, transform, device):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image = transform(image)\n",
    "    image = image.unsqueeze(0)  # 添加batch维度\n",
    "    image = image.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(image)\n",
    "        probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
    "        top_prob, top_catid = torch.topk(probabilities, 1)\n",
    "\n",
    "    # 反转label_to_index字典\n",
    "    index_to_label = {v: k for k, v in label_to_index.items()}\n",
    "\n",
    "    return index_to_label[top_catid.item()], top_prob.item()\n",
    "\n",
    "# 示例推理\n",
    "test_image_path = os.path.join('data', '1.jpg')\n",
    "predicted_class, probability = predict_image(test_image_path, model, transform, device)\n",
    "print(f\"Predicted class: {predicted_class}, Probability: {probability}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iapr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
